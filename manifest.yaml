#
# We are using a GCE disk as persistent storage for the Hub
# and eventually, each individual user node.
#
# An administrator with access must provision these disks with a reasonable amount of space
# before hand. Please note that with the K8s deployment, we are no longer using a shared NFS
# and instead each user will have his or her own disk.
#
# Find the region for your cluster beforehand using:
#
#     gcloud container clusters list
#
# Provision your disk like so:
#
#     gcloud compute disks create hub-workdir-01 --size 1GiB
#
# Please make sure to delete your disk after you are done using it as you will be charged.
#
#     gcloud compute disks delete hub-workdir-01

kind: ConfigMap
apiVersion: v1
metadata:
  name: config
# TODO: Make secrets more secure
data:
  # Used to authenticate the culler to the hub. This string was generated with `openssl rand -hex 32`.
  auth.jhub-token.cull: 13fdff1305cd883e49223908186a63294922dadb59b5d1122473041f160c4b03
  oauth.client-id.google: 92948014362-c7jc8k20co1e4eqmg8095818htadijat.apps.googleusercontent.com
  oauth.client-secret.google: BabUWSqHd4ZekBqiaur4S1cm
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: data8-hub-workdir-01
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: data8-hub-workdir-01
    fsType: ext4
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: data8-workdir
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: hub-proxy
spec:
  type: LoadBalancer
  selector:
    name: hub-proxy-pod
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hub-proxy-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: hub-proxy-pod
    spec:
      volumes:
      - name: data8-workdir-volume
        persistentVolumeClaim:
          claimName: data8-workdir
      containers:
      - name: hub-proxy-container
        image: data8/jupyterhub-k8s-hub:data8_jupyterhubv2
        volumeMounts:
          - mountPath: /srv/jupyterhub
            name: data8-workdir-volume
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CULL_JHUB_TOKEN
          valueFrom:
            configMapKeyRef:
              name: config
              key: auth.jhub-token.cull
        - name: GOOGLE_OAUTH_CLIENT_ID
          valueFrom:
            configMapKeyRef:
              name: config
              key: oauth.client-id.google
        - name: GOOGLE_OAUTH_CLIENT_SECRET
          valueFrom:
            configMapKeyRef:
              name: config
              key: oauth.client-secret.google
        - name: OAUTH_CALLBACK_URL
          value: http://derrick-jhub.calblueprint.org/hub/oauth_callback
        ports:
          - containerPort: 8000
            name: hub-proxy-port
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cull-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: cull-pod
    spec:
      containers:
        - name: cull-container
          image: data8/jupyterhub-k8s-cull:master
          command:
            - /bin/sh
            - -c
          args: ['python /srv/cull/cull_idle_servers.py --timeout=3600 --cull_every=600 --url=http://${HUB_PROXY_SERVICE_HOST}:${HUB_PROXY_SERVICE_PORT}/hub']
          env:
          - name: JPY_API_TOKEN
            valueFrom:
              configMapKeyRef:
                name: config
                key: auth.jhub-token.cull
